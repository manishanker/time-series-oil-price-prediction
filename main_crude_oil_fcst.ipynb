{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import datetime as dt\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import holidays\n",
    "import pandas_market_calendars as mcal   \n",
    "from openpyxl import load_workbook\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set display options for dataframes in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel ('raw_data.xlsx', index_col = 'dates')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate x and y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = df[['P_wti_oil']].copy()\n",
    "y_df.dropna(inplace = True)\n",
    "y_df_pct_chg = y_df.pct_change()\n",
    "y_df_pct_chg.dropna(inplace = True)\n",
    "x_df_raw = df.drop(['P_wti_oil'], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove columns with only 0s or NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = x_df_raw.columns\n",
    "\n",
    "for name in col_names:\n",
    "    \n",
    "    if any(x_df_raw[name].values>0) == False and any(x_df_raw[name].values<0) == False:\n",
    "        x_df_raw.drop(name, 1, inplace = True)\n",
    "        print(['column ' + name + ' was removed as there were only non-zero or Nan values.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate daily and weekly variables, put weekly variables in a dictionary containing two dataframes: one for positions reports and one for inventory report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_data = ['pos', 'inventory']\n",
    "dict_weekly_vars, dict_weekly_vars_pct_chg, dict_weekly_vars_1diff = {}, {}, {}\n",
    "weekly_cols = list()\n",
    "\n",
    "for name in weekly_data:\n",
    "    temp_col = [col for col in x_df_raw.columns if name in col]\n",
    "    dict_weekly_vars[name] = x_df_raw[temp_col].copy()\n",
    "    dict_weekly_vars[name].dropna(inplace = True)\n",
    "    dict_weekly_vars_pct_chg[name] = dict_weekly_vars[name].pct_change()\n",
    "    dict_weekly_vars_1diff[name] = dict_weekly_vars[name].diff()\n",
    "    weekly_cols.append(temp_col)\n",
    "\n",
    "weekly_cols = list(itertools.chain.from_iterable(weekly_cols))\n",
    "daily_cols = [col for col in x_df_raw.columns if col not in weekly_cols]\n",
    "x_df_raw_daily = x_df_raw[daily_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Net position for positions variables (long minus short) as that's what is useful to know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_raw = dict_weekly_vars['pos']\n",
    "\n",
    "df_pos_raw_net, df_pos_pct_chg_net, df_pos_diff_net = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "pos_cols = df_pos_raw.columns\n",
    "pos_cols = [x[:-1] for x in pos_cols]\n",
    "unique_cat = np.unique(pos_cols)\n",
    "for i in range(0,4):\n",
    "    df_pos_raw_net[pos_cols[i] + '_net'] = df_pos_raw.iloc[:,i] - df_pos_raw.iloc[:, i + 4]\n",
    "    df_pos_pct_chg_net[pos_cols[i] + '_net'] = df_pos_raw_net[pos_cols[i] + '_net'].pct_change()\n",
    "    df_pos_diff_net[pos_cols[i] + '_net'] = df_pos_raw_net[pos_cols[i] + '_net'].diff()\n",
    "    is_net_long = df_pos_raw_net[pos_cols[i] + '_net'] > 0\n",
    "    df_pos_raw_net[pos_cols[i] + '_is_long'] = np.full((len(df_pos_raw_net), 1), 0)\n",
    "    idx = np.where(is_net_long == True)\n",
    "    idx = idx[0]\n",
    "    col_nbr =  df_pos_raw_net.columns.get_loc(pos_cols[i] + '_is_long')\n",
    "    df_pos_raw_net.iloc[idx, col_nbr] = 1\n",
    "    \n",
    "# only keep net positions variables (remove short and long)\n",
    "dict_weekly_vars['pos'] = df_pos_raw_net\n",
    "dict_weekly_vars_pct_chg['pos'] = df_pos_pct_chg_net\n",
    "dict_weekly_vars_diff['pos'] = df_pos_diff_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shift x values by one day from y values as only closing data is available (and therefore usable to make prediction). It's priced in after closing time (the day after)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df_daily = x_df_raw_daily.iloc[0:-1].values\n",
    "new_idx = x_df_raw_daily.index[1:len(x_df_raw_daily)]\n",
    "x_df_daily = pd.DataFrame(x_df_daily, index=new_idx, columns=list(x_df_raw_daily.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace missing values by previous value for variables (in our dataset missing values are not due to an error but to the fact that the data point is not available   so we use the previous value instead as that is the piece of information available to the market at the time of the missing value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df_daily = x_df_daily.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rows with no y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.isin(x_df_daily.index, y_df.index)\n",
    "x_df_daily = x_df_daily.iloc[idx,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform data into % change and first difference to make it stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df_daily_pct_chg = x_df_daily.pct_change()\n",
    "x_df_daily_pct_chg.drop(x_df_daily_pct_chg.index[0], inplace = True)\n",
    "x_df_daily_diff = x_df_daily.diff()\n",
    "x_df_daily_diff.drop(x_df_daily_diff.index[0], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change dates for weekly data to the day they become known to market participants\n",
    " + +5 days for DOE\n",
    " + +3 days for CFCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_weekly_vars_pct_chg['pos'].index = dict_weekly_vars_pct_chg['pos'].index + pd.DateOffset(days=3)\n",
    "dict_weekly_vars['pos'].index = dict_weekly_vars['pos'].index + pd.DateOffset(days=3)\n",
    "dict_weekly_vars_pct_chg['inventory'].index = dict_weekly_vars_pct_chg['inventory'].index + pd.DateOffset(days=5) \n",
    "dict_weekly_vars['inventory'].index = dict_weekly_vars['inventory'].index + pd.DateOffset(days=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the usual weekly variables publication day is a bank holiday, set the publication day to the next market open day\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_vars = ['pos', 'inventory']\n",
    "for each_var in weekly_vars:\n",
    "    is_close_week_var =  dict_weekly_vars_pct_chg[each_var].index[~np.isin(dict_weekly_vars_pct_chg[each_var].index, y_df.index)]  \n",
    "    for each_date in is_close_week_var:\n",
    "        diff = y_df.index - each_date\n",
    "        min = diff[(diff > pd.to_timedelta(0))].min()\n",
    "        each_date_loc = dict_weekly_vars_pct_chg[each_var].index.get_loc(each_date)\n",
    "        dict_weekly_vars_pct_chg[each_var].index.values[each_date_loc] = each_date + min\n",
    "    dict_weekly_vars[each_var].index  = dict_weekly_vars_pct_chg[each_var].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df_daily_pct_chg_float = x_df_daily_pct_chg.select_dtypes('float64')\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "names = list(x_df_daily_pct_chg_float.columns)\n",
    "f, axes = plt.subplots(len(names), figsize=(10,100))\n",
    "f2, axes2 = plt.subplots(len(names), figsize=(10,100))\n",
    "i = 0\n",
    "\n",
    "for name in names:\n",
    "    axes[i].scatter(x=x_df_daily_pct_chg_float[name].index, y=x_df_daily_pct_chg_float[name].values)\n",
    "    axes[i].label_outer()\n",
    "    axes[i].set_xlabel('years')\n",
    "    axes[i].title.set_text(name)\n",
    "    \n",
    "    sns.boxplot(x=x_df_daily_pct_chg_float[name], ax=axes2[i])\n",
    "    axes2[i].label_outer()\n",
    "    axes2[i].set_xlabel('values')\n",
    "    axes2[i].title.set_text(name)        \n",
    "    i = i + 1\n",
    "    \n",
    "f.tight_layout(pad = 5.0)\n",
    "f2.tight_layout(pad = 5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for outliers programmaticaly - return dataframe containing outlier analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_outliers = {}\n",
    "# set a threshold for the number of standard deviations above which the value is considered as an outlier\n",
    "THRESHOLD = 6\n",
    "\n",
    "for y in df.columns:\n",
    "    # using 365 days rolling average to calculate z_score as data isn't stationary\n",
    "    roll_mean = df[y].rolling(365, center=True, min_periods=1).mean()\n",
    "    roll_std = df[y].rolling(365, center=True, min_periods=1).std()\n",
    "    dict_outliers[y] = df[[y]].copy()\n",
    "    dict_outliers[y]['roll_mean'] = roll_mean.values\n",
    "    dict_outliers[y]['roll_std'] = roll_std.values\n",
    "    dict_outliers[y]['z_scores'] = abs(dict_outliers[y][y].values - dict_outliers[y]['roll_mean'])/dict_outliers[y]['roll_std']\n",
    "    dict_outliers[y]['is_outlier'] = False\n",
    "    \n",
    "    idx2 = np.where(dict_outliers[y]['z_scores'].values > THRESHOLD)\n",
    "    idx2 = idx2[0]\n",
    "    dict_outliers[y].loc[dict_outliers[y].index[idx2], 'is_outlier'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rows containing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keys = list(dict_outliers.keys())\n",
    "rows_to_remove = dict_outliers[all_keys[0]]['is_outlier'].index[dict_outliers[all_keys[0]]['is_outlier'] == True]\n",
    "\n",
    "for key in all_keys[1:]:\n",
    "    rows_to_remove = rows_to_remove.append(dict_outliers[key]['is_outlier'].index[dict_outliers[key]['is_outlier'] == True])\n",
    "\n",
    "rows_to_remove.drop_duplicates(keep='first')\n",
    "\n",
    "# remove outliers in x and y vars in pct as well as raw x and y vars (original unit)\n",
    "\n",
    "x_df_daily_pct_chg.drop(x_df_daily_pct_chg[x_df_daily_pct_chg.index.isin(rows_to_remove)].index, inplace = True)\n",
    "x_df_daily.drop(x_df_daily_pct_chg[x_df_daily_pct_chg.index.isin(rows_to_remove)].index, inplace = True)\n",
    "y_df_pct_chg.drop(y_df_pct_chg[~y_df_pct_chg.index.isin(x_df_daily_pct_chg.index)].index, inplace = True)\n",
    "y_df.drop(x_df_daily_pct_chg[x_df_daily_pct_chg.index.isin(rows_to_remove)].index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for stationarity - unit root test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df_daily_pct_chg_adf = x_df_daily_pct_chg.copy()\n",
    "x_df_daily_pct_chg_adf = x_df_daily_pct_chg_adf.select_dtypes('float64')   \n",
    "for name in x_df_daily_pct_chg.columns:\n",
    "    x_serie_daily_pct_chg = x_df_daily_pct_chg_adf[name].dropna()\n",
    "    try:\n",
    "        df_test = adfuller(x_serie_daily_pct_chg, autolag='AIC')\n",
    "        df_result = pd.Series(df_test[0:4], index=['t-stat','p-value','nbr-lags','nbr-obs'])\n",
    "        for key,value in df_test[4].items():\n",
    "            df_result['Critical Value (%s)'%key] = value\n",
    "            if df_result['t-stat'] > df_test[4]['1%']:\n",
    "                print(['Variable ' + name + ' data is not stationary: Test Statistic = ' + str(dftest[0]) + '; Critical Value 1% = ' + str(dftest[4]['1%'])])\n",
    "    except:\n",
    "        the_type, the_value, the_traceback = sys.exc_info()\n",
    "        print(['Error trying DFT with ' + name + ' ' + str(the_type) + '. ' + str(the_value)])\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create lag variables and moving average variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to create lag and mvg average variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_mov_avg_var(x_df, get_mvg_avg = False):  \n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a dataframe containing lagged variables and moving average variables:\n",
    "    - lagged variables from 1 to 4 periods (days if input dataframe is in days, week if input dataframe is in weeks)\n",
    "    - moving averages variables over 2 to 10 days (only if get_mvg_avg is set to True)\n",
    "    \"\"\"\n",
    "    \n",
    "    NUMBER_LAGS = 4\n",
    "    \n",
    "    dict_lag_mvavg_vars = {}\n",
    "    \n",
    "    for lag in range(1, NUMBER_LAGS + 1):\n",
    "        temp_val = x_df.iloc[0:-lag].values\n",
    "        temp_index = x_df.index[lag:len(x_df)]\n",
    "        dict_lag_mvavg_vars['x_df_daily_lag_' + str(lag)] = pd.DataFrame(temp_val, index=temp_index, columns=list(x_df.columns))\n",
    "        dict_lag_mvavg_vars['x_df_daily_lag_' + str(lag)] = dict_lag_mvavg_vars['x_df_daily_lag_' + str(lag)].add_suffix('_lag_' + str(lag) + '_p')\n",
    "        \n",
    "    # get_mvg_avg is a boolean: true for daily variables for which we want to calculate moving averages \n",
    "    # and False for weekly variables for which we don't calculate moving average\n",
    "    \n",
    "    if get_mvg_avg:\n",
    "        # create 2-10 days moving averages\n",
    "        \n",
    "        ROLLING_DAYS = 10\n",
    "        \n",
    "        for rolling_day in range(2, ROLLING_DAYS + 1):\n",
    "            dict_lag_mvavg_vars['x_df_rolling_' + str(rolling_day)] = x_df.rolling(rolling_day, min_periods=1).mean()\n",
    "            dict_lag_mvavg_vars['x_df_rolling_' + str(rolling_day)] = dict_lag_mvavg_vars['x_df_rolling_' + str(rolling_day)].add_suffix('_rolling_avg_' + str(rolling_day) + '_p')\n",
    "    \n",
    "    x_df_lag = pd.DataFrame(index = x_df.index)\n",
    "    \n",
    "    for key in dict_lag_mvavg_vars.keys():\n",
    "        x_df_lag = pd.merge(x_df_lag, dict_lag_mvavg_vars[key], how='left', left_index=True, right_index=True)    \n",
    "        \n",
    "    return x_df_lag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the function create_lag_mov_avg_var() on daily variables dataframes (variables in pct: x_df_daily_pct_chg and raw values: x_df_daily) and weekly variables (in pct: dict_weekly_vars_pct_chg and raw values: dict_weekly_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df_lag_pct_chg = create_lag_mov_avg_var(x_df_daily_pct_chg, get_mvg_avg = True)\n",
    "x_df_lag_daily = create_lag_mov_avg_var(x_df_daily, get_mvg_avg = True)\n",
    "\n",
    "dict_weekly_lag_pct_chg, dict_weekly_lag = {}, {}\n",
    "\n",
    "for key in dict_weekly_vars_pct_chg.keys():\n",
    "    # add 1 to all weekly percentage change as when weekly variables are merged with daily variable, NaN on the days weekly variables are missing will be replaced by 0\n",
    "    # we need to add 1 to weekly percentage changes so inputted values and actual values are easily differentiable\n",
    "    dict_weekly_vars_pct_chg[key] = dict_weekly_vars_pct_chg[key] + 1\n",
    "    dict_weekly_lag_pct_chg[key] = create_lag_mov_avg_var(dict_weekly_vars_pct_chg[key], get_mvg_avg = False)\n",
    "    dict_weekly_lag[key] = create_lag_mov_avg_var(dict_weekly_vars[key], get_mvg_avg = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create calendar variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cal_vars(x_df, y_df, dict_input):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a dataframe containing the following variables, all hot encoded as dummies:\n",
    "    - day of the month (0-30) \n",
    "    - week of the month (0-4) \n",
    "    - day of the week (0-6)\n",
    "    - month (1-12)\n",
    "    - year (2000-2020)\n",
    "    - days before or after bank holidays\n",
    "    The function also merge daily variables with weekly variables.\n",
    "    \"\"\"\n",
    "\n",
    "    temp_df = pd.DataFrame()\n",
    "    temp_df['day_of_month'] = x_df.index.day\n",
    "    temp_df['week_of_month'] = (x_df.index.day - 1) // 7 + 1\n",
    "    temp_df['day_of_week'] = x_df.index.weekday\n",
    "    temp_df['month'] = x_df.index.month\n",
    "    temp_df['year'] = x_df.index.year\n",
    "    temp_df = temp_df.set_index(x_df.index)\n",
    "    \n",
    "    # transform all categorical variables as 1/0 dummies\n",
    "    for col in temp_df.columns:\n",
    "        temp_df[col] = temp_df[col].astype('category')\n",
    "        hot_enc = pd.get_dummies(temp_df, prefix= col + '_')\n",
    "        hot_enc = hot_enc.set_index(x_df.index)#\n",
    "        x_df_cal = hot_enc.copy()\n",
    "        #x_df = pd.merge(x_df, hot_enc, how='left', left_index=True, right_index=True)  \n",
    "        \n",
    "    #Find which days are closed bank holidays for NYMEX so we can see which days in the y dataset preceeds a bank holiday\n",
    "    # Get all business dates (no weekend) between the dataset start date and end date\n",
    "    \n",
    "    start_date = x_df.index.min().to_pydatetime()\n",
    "    end_date = x_df.index.max().to_pydatetime()\n",
    "    all_business_dates = pd.date_range(start_date, end_date, freq='B').to_pydatetime()\n",
    "    \n",
    "    # workout which days in the business dates is a bank holiday (NYMEX was closed so no y data on those days)\n",
    "    NYMEX_open_days = y_df.index.to_pydatetime()\n",
    "    is_close = ~np.isin(all_business_dates.astype('datetime64[ns]'), NYMEX_open_days.astype('datetime64[ns]'))\n",
    "    \n",
    "    # crete a dummy to identify bank holidays (0 if not a bank holiday and 1 if it's a bank holiday)\n",
    "    temp_df2 = pd.DataFrame(np.full((len(all_business_dates), 4), 0), index=all_business_dates, \\\n",
    "                            columns=['is_trading_days_before_bank_holiday', 'is_days_before_xmas', \\\n",
    "                                     'is_days_after_xmas', 'is_days_after_NY'])\n",
    "    idx_close = np.where(is_close==True)\n",
    "    idx_close = idx_close[0]\n",
    "    \n",
    "    # identify the 3 days preceeding a bank holiday with a dummy \n",
    "    # (0 if not a day preceding a bank holiday - NYMEX close days - \n",
    "    # and 1 if the days are within 3 days before a bank holiday)\n",
    "    \n",
    "    DAYS_AROUND_BANK_HOL = 3\n",
    "    \n",
    "    for x in idx_close:\n",
    "        temp_df2['is_trading_days_before_bank_holiday'].iloc[x - DAYS_AROUND_BANK_HOL + 1 : x] = 1\n",
    "        # identify the 3 days before and after xmas and the 3 days after new year\n",
    "        year = temp_df2.index[x].year\n",
    "        \n",
    "        if temp_df2.index[x].to_pydatetime == dt.datetime(year, 12, 25):\n",
    "            temp_df2['is_days_before_xmas'].iloc[idx - DAYS_AROUND_BANK_HOL : idx + 1] = 1\n",
    "            temp_df2['is_days_after_xmas'].iloc[idx : idx + DAYS_AROUND_BANK_HOL + 1] = 1\n",
    "            \n",
    "        elif temp_df2.index[x].to_pydatetime == dt.datetime(year, 12, 25):\n",
    "             temp_df2['is_days_after_NY'].iloc[idx : idx + DAYS_AROUND_BANK_HOL + 1] = 1\n",
    "                \n",
    "    x_df_cal = pd.merge(x_df_cal, temp_df2, how='left', left_index = True, right_index = True)\n",
    "    \n",
    "    # add x weekly variables to x daily variables to put back all x variables together\n",
    "    weekly_var = ['pos', 'inventory']\n",
    "    \n",
    "    for each_var in weekly_var:  \n",
    "        x_df = pd.merge(x_df, dict_input[each_var], how='left', left_index=True, right_index=True)\n",
    "        \n",
    "    # add a dummy to indicate whether the weekly data is available on a day or not\n",
    "    x_df['is_pos_data'] = np.full((len(x_df), 1), 0)\n",
    "    x_df['is_pos_data'][~np.isnan(x_df['pos_oil_prod_lon_net'])] = 1\n",
    "    x_df['is_inventory_data'] = np.full((len(x_df), 1), 0)\n",
    "    x_df['is_inventory_data'][~np.isnan(x_df['crude_inventory'])] = 1\n",
    "    \n",
    "    # Replace NaNs by 0 in missing weekly daily variables:\n",
    "    for key in dict_input.keys():\n",
    "        \n",
    "        for col in dict_input[key].columns:\n",
    "            x_df[col] = x_df[col].fillna(0)\n",
    "            \n",
    "    return x_df_cal, x_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create calendar variables for x vars dataframe in pct and x vars dataframe with raw values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df_cal_pct_chg, x_df_pct_chg = create_cal_vars(x_df_daily_pct_chg, y_df_pct_chg, dict_weekly_vars_pct_chg)\n",
    "x_df_cal, x_df_daily = create_cal_vars(x_df_daily, y_df, dict_weekly_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create products variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that outputs product variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prod_vars(x_df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a dataframe containing product variables as follow for price variables:\n",
    "    - volume & price\n",
    "    - open interest & price\n",
    "    - volume, open interest & price\n",
    "    \"\"\"\n",
    "\n",
    "    new_cols = [col.replace('P_', '') for col in x_df.columns if 'P_' in col]\n",
    "\n",
    "    product_vars = pd.DataFrame(index = x_df.index)\n",
    "\n",
    "    for new_col in new_cols:\n",
    "        product_vars[new_col + '_V_P'] = x_df['V_' + new_col] * x_df['P_' + new_col]\n",
    "        product_vars[new_col + '_OI_P'] = x_df['OI_' + new_col] * x_df['P_' + new_col]\n",
    "        product_vars[new_col + '_V_OI_P'] = x_df['V_' + new_col] * x_df['OI_' + new_col] * x_df['P_' + new_col]\n",
    "\n",
    "    return product_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the create_prod_vars on x dataframes (pct data and raw data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_vars_pct_chg = create_prod_vars(x_df_daily_pct_chg)\n",
    "product_vars = create_prod_vars(x_df_daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
